"""
ç»Ÿä¸€ LLM å®¢æˆ·ç«¯

LLMClient æ˜¯ AI éª¨å¹²å±‚å¯¹å¤–çš„ä¸»è¦ LLM æ¥å£:
  - stream()   â†’ æµå¼ç”Ÿæˆ (yield LLMEvent)
  - complete() â†’ éæµå¼ç”Ÿæˆ
  - embed()    â†’ Embedding

å®ƒè´Ÿè´£:
  1. Provider è·¯ç”±: æ ¹æ® model_id å‰ç¼€è§£æå‡ºæ­£ç¡®çš„ Provider
  2. æ¨ç†æ¨¡å‹æ£€æµ‹: auto åˆ‡æ¢ reasoning model å‚æ•°
  3. æ¶ˆæ¯æ ¼å¼è½¬æ¢: å†…éƒ¨æ ¼å¼ â†’ API æ ¼å¼
  4. é‡è¯•ç­–ç•¥: auth error â†’ é‡æ–°è·å– token, context overflow â†’ æŠ›å‡ºå¯æ¢å¤å¼‚å¸¸
  5. Provider æ± ç®¡ç†: å¤ç”¨ httpx.AsyncClient

**ä¸å« tool-calling loop** â€” çº¯ LLM é€šä¿¡å±‚ã€‚å·¥å…·å¾ªç¯ç”± Agent å±‚ç®¡ç†ã€‚
"""
from __future__ import annotations

import base64
import hashlib
import logging
import mimetypes
import time
import uuid
from typing import Any, AsyncGenerator, Dict, List, Optional, Set

from .providers.base import (
    BaseProvider,
    CompletionResult,
    ContextOverflowError,
    EmbeddingResult,
    EventType,
    ProviderCapability,
    ProviderError,
    ProviderEvent,
    ProviderInfo,
)
from .providers.github_models import GitHubModelsProvider, _parse_error_meta
from .providers.copilot import CopilotProvider
from .providers.openai_compat import OpenAICompatProvider

logger = logging.getLogger(__name__)

# æ¨ç†æ¨¡å‹å‰ç¼€
_REASONING_MODEL_PREFIXES = ("o1", "o3", "o4")

# Copilot å‰ç¼€
COPILOT_PREFIX = "copilot:"


def _is_reasoning_model(model: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºæ¨ç†æ¨¡å‹"""
    name = model.lower().removeprefix(COPILOT_PREFIX.lower())
    for prefix in _REASONING_MODEL_PREFIXES:
        if name == prefix or name.startswith(prefix + "-"):
            return True
    return False


def new_request_id() -> str:
    """æ¯æ¬¡ç”¨æˆ·æ¶ˆæ¯ç”Ÿæˆæ–°çš„ request_id (è®¡è´¹å½’é›†)"""
    rid = str(uuid.uuid4())
    logger.info(f"æ–°æ¶ˆæ¯ request_id: {rid[:8]}...")
    return rid


# â”€â”€ LLM äº‹ä»¶ (å¯¹ ProviderEvent çš„ä¸Šå±‚å°è£…) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class LLMEvent:
    """ç»Ÿä¸€ LLM äº‹ä»¶ â€” ç›´æ¥å¤ç”¨ ProviderEvent ç»“æ„ä½†æ›´è¯­ä¹‰åŒ–"""
    __slots__ = ("type", "data")

    def __init__(self, type: str, **data):
        self.type = type
        self.data = data

    def __repr__(self):
        return f"LLMEvent({self.type}, {self.data})"

    def to_dict(self) -> Dict[str, Any]:
        """è½¬ä¸ºå‘åå…¼å®¹çš„ SSE dict (ä¸æ—§ ai_service äº‹ä»¶åè®®ä¸€è‡´)"""
        d = {"type": self.type}
        d.update(self.data)
        return d


# â”€â”€ LLMClient â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class LLMClient:
    """ç»Ÿä¸€ LLM å®¢æˆ·ç«¯

    ä½¿ç”¨:
      client = LLMClient.get_instance()
      async for event in client.stream(messages, model, system_prompt=...):
          ...
    """

    _instance: Optional["LLMClient"] = None

    def __init__(self):
        self._providers: Dict[str, BaseProvider] = {}  # cache_key â†’ Provider
        self._provider_cache_ts: float = 0
        self._CACHE_TTL = 60

    @classmethod
    def get_instance(cls) -> "LLMClient":
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    # â”€â”€ Provider è·¯ç”± â”€â”€

    async def _resolve_provider(self, model_id: str) -> tuple[BaseProvider, str]:
        """è§£æ model_id â†’ (Provider å®ä¾‹, actual_model_name)

        model_id æ ¼å¼:
          - "gpt-4o"                â†’ GitHub Models
          - "copilot:gpt-4o"        â†’ Copilot API
          - "deepseek:deepseek-chat" â†’ ç¬¬ä¸‰æ–¹ (DB æŸ¥è¯¢)
        """
        from studio.backend.core.config import settings
        from studio.backend.services.copilot_auth import COPILOT_CHAT_URL

        now = time.time()
        cache_stale = (now - self._provider_cache_ts) > self._CACHE_TTL

        # Copilot
        if model_id.startswith(COPILOT_PREFIX):
            actual = model_id[len(COPILOT_PREFIX):]
            cache_key = "copilot"
            if cache_key not in self._providers or cache_stale:
                info = ProviderInfo(
                    provider_type="copilot", slug="copilot",
                    actual_model=actual, base_url=COPILOT_CHAT_URL,
                    icon="â˜ï¸", name="Copilot",
                )
                self._providers[cache_key] = CopilotProvider(info)
                self._provider_cache_ts = now
            return self._providers[cache_key], actual

        # ç¬¬ä¸‰æ–¹: slug:model æ ¼å¼
        if ":" in model_id:
            slug, actual = model_id.split(":", 1)
            cache_key = slug

            if cache_key in self._providers and not cache_stale:
                return self._providers[cache_key], actual

            from studio.backend.api.provider_api import get_provider_by_slug
            provider_row = await get_provider_by_slug(slug)
            if provider_row and provider_row.enabled:
                info = ProviderInfo(
                    provider_type=provider_row.provider_type,
                    slug=provider_row.slug,
                    actual_model=actual,
                    base_url=provider_row.base_url,
                    api_key=provider_row.api_key or "",
                    icon=provider_row.icon,
                    name=provider_row.name,
                )
                self._providers[cache_key] = OpenAICompatProvider(info)
                self._provider_cache_ts = now
                return self._providers[cache_key], actual
            else:
                logger.warning(f"æä¾›å•† '{slug}' ä¸å­˜åœ¨æˆ–æœªå¯ç”¨, å›é€€åˆ° GitHub Models")

        # é»˜è®¤: GitHub Models
        cache_key = "github"
        if cache_key not in self._providers or cache_stale:
            from studio.backend.api.provider_api import get_provider_by_slug
            provider_row = await get_provider_by_slug("github")
            api_key = ((provider_row.api_key if provider_row else "") or settings.github_token or "").strip()
            info = ProviderInfo(
                provider_type="github_models", slug="github",
                actual_model=model_id,
                base_url=settings.github_models_endpoint,
                api_key=api_key,
                icon="ğŸ™", name="GitHub Models",
            )
            self._providers[cache_key] = GitHubModelsProvider(info)
            self._provider_cache_ts = now
        return self._providers[cache_key], model_id

    def invalidate_cache(self):
        """æ¸…é™¤ provider ç¼“å­˜ (é…ç½®å˜æ›´åè°ƒç”¨)"""
        self._providers.clear()
        self._provider_cache_ts = 0

    # â”€â”€ æ¶ˆæ¯æ„å»º â”€â”€

    @staticmethod
    def build_api_messages(
        messages: List[Dict[str, Any]],
        system_prompt: str = "",
        is_reasoning: bool = False,
    ) -> List[Dict[str, Any]]:
        """å°†å†…éƒ¨æ¶ˆæ¯æ ¼å¼è½¬ä¸º OpenAI API æ ¼å¼"""
        api_messages: List[Dict[str, Any]] = []

        if system_prompt:
            if is_reasoning:
                api_messages.append({
                    "role": "user",
                    "content": f"[System Instructions]\n{system_prompt}",
                })
            else:
                api_messages.append({"role": "system", "content": system_prompt})

        for msg in messages:
            role = msg["role"]
            content = msg.get("content", "")
            images = msg.get("images", [])

            if role == "tool":
                api_messages.append({
                    "role": "tool",
                    "tool_call_id": msg.get("tool_call_id", ""),
                    "content": msg.get("content", ""),
                })
                continue

            if role == "assistant" and "tool_calls" in msg:
                entry: Dict[str, Any] = {"role": "assistant"}
                entry["content"] = content if content else None
                entry["tool_calls"] = msg["tool_calls"]
                api_messages.append(entry)
                continue

            if images and role == "user":
                content_parts: List[Dict[str, Any]] = []
                if content:
                    content_parts.append({"type": "text", "text": content})
                for img in images:
                    content_parts.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{img['mime_type']};base64,{img['base64']}"
                        },
                    })
                api_messages.append({"role": role, "content": content_parts})
            else:
                api_messages.append({"role": role, "content": content})

        return api_messages

    # â”€â”€ æ ¸å¿ƒæ¥å£ â”€â”€

    async def stream(
        self,
        messages: List[Dict[str, Any]],
        model: str = "gpt-4o",
        *,
        system_prompt: str = "",
        temperature: float = 0.7,
        max_tokens: int = 8192,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: str = "auto",
        request_id: str = "",
    ) -> AsyncGenerator[LLMEvent, None]:
        """æµå¼ LLM è°ƒç”¨ â€” yield LLMEvent

        ä¸å« tool-calling loopã€‚å•æ¬¡ LLM è°ƒç”¨ï¼Œè¿”å›å†…å®¹ + tool_callsã€‚
        å·¥å…·å¾ªç¯ç”± Agent å±‚ç®¡ç†ã€‚
        """
        provider, actual_model = await self._resolve_provider(model)
        is_reasoning = _is_reasoning_model(actual_model)

        # è®¤è¯æ£€æŸ¥
        auth_error = self._check_auth(provider, model)
        if auth_error:
            yield LLMEvent("error", error=auth_error)
            return

        # æ¨ç†æ¨¡å‹: ç¦ç”¨ tools, ä½¿ç”¨ complete() éæµå¼
        if is_reasoning:
            if tools:
                logger.info(f"æ¨ç†æ¨¡å‹ {actual_model} ä¸æ”¯æŒ tools, è·³è¿‡å·¥å…·æ³¨å…¥")
                tools = None
            async for ev in self._stream_reasoning(provider, messages, actual_model, system_prompt, max_tokens, request_id):
                yield ev
            return

        # æ„å»º API æ¶ˆæ¯
        api_messages = self.build_api_messages(messages, system_prompt, False)

        # æµå¼è°ƒç”¨
        async for pev in provider.stream(
            api_messages, actual_model,
            temperature=temperature, max_tokens=max_tokens,
            tools=tools, tool_choice=tool_choice,
            request_id=request_id,
        ):
            yield self._convert_provider_event(pev)

    async def _stream_reasoning(
        self,
        provider: BaseProvider,
        messages: List[Dict[str, Any]],
        actual_model: str,
        system_prompt: str,
        max_tokens: int,
        request_id: str,
    ) -> AsyncGenerator[LLMEvent, None]:
        """æ¨ç†æ¨¡å‹: éæµå¼è°ƒç”¨, åŒ…è£…ä¸º LLMEvent åºåˆ—"""
        api_messages = self.build_api_messages(messages, system_prompt, True)
        try:
            result = await provider.complete(
                api_messages, actual_model,
                max_tokens=max_tokens,
                request_id=request_id,
            )
        except ProviderError as e:
            yield LLMEvent("error", error=str(e), error_meta=e.error_meta)
            return

        if result.thinking:
            yield LLMEvent("thinking", content=result.thinking)
        if result.content:
            yield LLMEvent("content", content=result.content)
        if result.usage:
            yield LLMEvent("usage", usage={
                "prompt_tokens": result.usage.get("prompt_tokens", 0),
                "completion_tokens": result.usage.get("completion_tokens", 0),
                "total_tokens": result.usage.get("total_tokens", 0),
                "reasoning_tokens": result.usage.get("completion_tokens_details", {}).get("reasoning_tokens", 0),
            })

    async def complete(
        self,
        messages: List[Dict[str, Any]],
        model: str = "gpt-4o",
        *,
        system_prompt: str = "",
        temperature: float = 0.7,
        max_tokens: int = 8192,
    ) -> str:
        """éæµå¼ LLM è°ƒç”¨ â€” è¿”å›çº¯æ–‡æœ¬å†…å®¹"""
        result_parts: List[str] = []
        async for event in self.stream(
            messages, model,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
        ):
            if event.type == "content":
                result_parts.append(event.data.get("content", ""))
            elif event.type == "error":
                result_parts.append(event.data.get("error", ""))
        return "".join(result_parts)

    async def embed(
        self,
        texts: List[str],
        model: str = "text-embedding-3-small",
        *,
        provider_slug: str = "github",
    ) -> EmbeddingResult:
        """Embedding è°ƒç”¨

        Args:
            texts: å¾… embed çš„æ–‡æœ¬åˆ—è¡¨
            model: embedding æ¨¡å‹å
            provider_slug: ä½¿ç”¨å“ªä¸ªæä¾›å•† (é»˜è®¤ github)
        """
        # æ„é€ ä¸€ä¸ªä¸´æ—¶ model_id æ¥è§£æ provider
        model_id = f"{provider_slug}:{model}" if provider_slug != "github" else model
        provider, _ = await self._resolve_provider(model_id)
        return await provider.embed(texts, model)

    # â”€â”€ å†…éƒ¨å·¥å…· â”€â”€

    def _check_auth(self, provider: BaseProvider, model: str) -> str:
        """æ£€æŸ¥è®¤è¯çŠ¶æ€, è¿”å›é”™è¯¯æ¶ˆæ¯æˆ–ç©ºå­—ç¬¦ä¸²"""
        if isinstance(provider, CopilotProvider):
            from studio.backend.services.copilot_auth import copilot_auth
            if not copilot_auth.is_authenticated:
                return "âŒ æœªæˆæƒ Copilotï¼Œè¯·åœ¨è®¾ç½®é¡µé¢å®Œæˆ OAuth æˆæƒ"
        elif isinstance(provider, GitHubModelsProvider):
            if not provider.info.api_key:
                return "âŒ æœªé…ç½® GitHub Models å…¨å±€ Tokenï¼Œè¯·åœ¨ AI æœåŠ¡è®¾ç½®ä¸­é…ç½®"
        elif isinstance(provider, OpenAICompatProvider):
            if not provider.info.api_key:
                return f"âŒ {provider.info.name} æœªé…ç½® API Keyï¼Œè¯·åœ¨ AI æœåŠ¡è®¾ç½®ä¸­é…ç½®"
        return ""

    @staticmethod
    def _convert_provider_event(pev: ProviderEvent) -> LLMEvent:
        """å°† ProviderEvent è½¬ä¸º LLMEvent"""
        if pev.type == EventType.CONTENT_DELTA:
            return LLMEvent("content", content=pev.text)
        elif pev.type == EventType.THINKING_DELTA:
            return LLMEvent("thinking", content=pev.text)
        elif pev.type == EventType.TOOL_CALL_DELTA:
            return LLMEvent("tool_call_delta",
                            tool_call_index=pev.tool_call_index,
                            tool_call_id=pev.tool_call_id,
                            name=pev.name,
                            arguments_delta=pev.arguments_delta)
        elif pev.type == EventType.USAGE:
            return LLMEvent("usage", usage=pev.usage)
        elif pev.type == EventType.FINISH:
            return LLMEvent("finish", finish_reason=pev.finish_reason)
        elif pev.type == EventType.ERROR:
            return LLMEvent("error", error=pev.error, error_meta=pev.error_meta)
        else:
            return LLMEvent("unknown", raw=str(pev))

    # â”€â”€ ç”Ÿå‘½å‘¨æœŸ â”€â”€

    async def close(self):
        """å…³é—­æ‰€æœ‰ Provider è¿æ¥"""
        for p in self._providers.values():
            await p.close()
        self._providers.clear()


# â”€â”€ ä¾¿æ·å‡½æ•° (æ¨¡å—çº§) â”€â”€

def get_llm_client() -> LLMClient:
    """è·å–å…¨å±€ LLMClient å•ä¾‹"""
    return LLMClient.get_instance()


# â”€â”€ å›¾ç‰‡å·¥å…· (ä»æ—§ ai_service è¿å…¥) â”€â”€

def encode_image_to_base64(file_bytes: bytes) -> str:
    """å°†å›¾ç‰‡å­—èŠ‚ç¼–ç ä¸º base64"""
    return base64.b64encode(file_bytes).decode("utf-8")


def get_mime_type(filename: str) -> str:
    """æ ¹æ®æ–‡ä»¶åè·å– MIME ç±»å‹"""
    mime_type, _ = mimetypes.guess_type(filename)
    return mime_type or "image/png"
